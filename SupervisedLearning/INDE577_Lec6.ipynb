{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Goal: Given student informaion (such as GMAT score and SAT score), we have been asked to predictadmission to Rice)\n",
    "\n",
    "Given: Historical admissions records\n",
    "\n",
    "cols=(Student, GMAT, SAT, Rice(0,1))\n",
    "\n",
    "## Supervised Learning Task\n",
    "* Binary Classification\n",
    "* Pridect Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "class SingleNeuron:\n",
    "    def __init__(self, import_size, activation_function, loss, bias=True):      (attributes above)\n",
    "        self.input_size = input_size\n",
    "        self.ativation_function\n",
    "        self.loss = loss\n",
    "        self.bias = True\n",
    "        self.w = np.random.rand(self.input_size)\n",
    "        if self.bias == True:\n",
    "            self.b = np.random.rand()[0]\n",
    "    \n",
    "    def feed_forward(self, x):\n",
    "        if self.bias == True:\n",
    "            z = self.w @ x + self.b\n",
    "            return self.activation_function(z)\n",
    "        else:\n",
    "            z = self.w @ x\n",
    "            return self.activation_function(z)\n",
    "  \n",
    "P = SingleNeuron(z, 'sigmoid', 'MSE')\n",
    "P.loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sigmoid\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "# Define loss\n",
    "def cross_entropy_loss(x, y, network):\n",
    "    y_hat = network.feed_forward(x)\n",
    "    return -y*np.log(y_hat) - (1 - y)*np.log(1 - y_hat)\n",
    "\n",
    "# need the gradient?\n",
    "def cross_entropy_gradient(x, y, network):\n",
    "    y_hat = network.feed_forword(x)\n",
    "    dw = (y_hat - y)*x\n",
    "    if network.bias == True:\n",
    "        db = (y_hat - y)\n",
    "        return dw, db\n",
    "    else:\n",
    "        return dw\n",
    "\n",
    "# doing gradient descent\n",
    "def stochastic_gradient_descent(X,\n",
    "                                Y, \n",
    "                                network, \n",
    "                                learning_rate = 0.04,\n",
    "                                epochs = 10_000):\n",
    "    number_of_examples = len(y)\n",
    "    for _ in range(len(epochs)):\n",
    "        random_index = np.random.randint(number_of_examples)\n",
    "        x = X[random_index]\n",
    "        y = Y[random_index]\n",
    "        if network.bias == True:\n",
    "            dw, db = cross_entropy_gradient(x, y, network)\n",
    "            network.w -= learning_rate*dw\n",
    "            network.b -= learning_rate*db\n",
    "        else:\n",
    "            dw = cross_entropy_loss(x, y, network)\n",
    "            network.w -= learning_rate*dw\n",
    "            \n",
    "    total_loss = sum(cross_entropy_loss(X[i], Y[i], network)\n",
    "                    for i in range(number_of_examples))\n",
    "    \n",
    "    print(f\"Total Current Loss = {total_loss}\")\n",
    "\n",
    "class SigleNeuron:\n",
    "    def _init__(self,\n",
    "                input_size, \n",
    "                activation_function = sigmoid,\n",
    "                loss = cross_entropy_loss,\n",
    "                bias=True):\n",
    "        self.input_size = input_size\n",
    "        self.ativation_function = activation_function\n",
    "        self.loss = loss\n",
    "        self.bias = bias\n",
    "        self.w = np.random.rand(self.input_size)\n",
    "        if self.bias == True:\n",
    "            self.b = np.random.rand()[0]\n",
    "            \n",
    "    def feed_forward(self, x):\n",
    "        if self.bias == True:\n",
    "            z = self.w @ x + self.b\n",
    "            return self.activation_function(z)\n",
    "        else:\n",
    "            z = self.w @ x\n",
    "            return self.activation_function(z)\n",
    "        \n",
    "def fit():\n",
    "    stochastic_gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
